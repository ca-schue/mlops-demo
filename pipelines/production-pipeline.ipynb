{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git + DVC clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import Output, Dataset, Model, Artifact\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image='python:3.9.16',packages_to_install=[\"subprocess.run\",\"dvc[s3]\"])\n",
    "def git_dvc_clone(\n",
    "    repo: Output[Artifact],\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    import subprocess, os, shutil\n",
    "\n",
    "    gl_at = os.environ['GITLAB_ACCESS_TOKEN']\n",
    "    repo_name = \"ds-seminar-mlops-demo\"\n",
    "\n",
    "    subprocess.call([\"git\", \"clone\", \"https://<Git User>:\" + gl_at + \"@gitlab.com/<Git User>/\" + repo_name + \".git\"])\n",
    "    subprocess.call([\"git\", \"pull\"], cwd=\"/\" + repo_name)\n",
    "    shutil.copytree(\"/\" + repo_name, repo.path)\n",
    "\n",
    "    subprocess.call([\"dvc\", \"pull\"], cwd=\"/\" + repo_name)\n",
    "\n",
    "    os.rename(\"/\" + repo_name + \"/data.csv\", dataset.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import Input, Output, Dataset\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image='python:3.9.16',packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
    "def data_preprocessing(\n",
    "    dataset: Input[Dataset],\n",
    "    processed_dataset: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    ## Data Storage\n",
    "    # Einlesen Datensatz \"auto_mpg\" aus aktuellem Ordner \n",
    "    df = pd.read_csv(dataset.path)\n",
    "\n",
    "\n",
    "    ## Data Cleaning\n",
    "    # Entfernung der nominaler Merkmale für die Regression \n",
    "    df = df.drop(columns=['name'])\n",
    "\n",
    "    # Umwandlung in numerische Datentypen, löschen inkompatibler Daten\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "    # Entfernen von Ausreißerdaten außerhalb von Mittelwert +/- 3 Standardabweichung\n",
    "    for col in df.columns:\n",
    "        df = df.drop(df[(df[col] < (df[col].mean() - 2.5*df[col].std())) \n",
    "                    | (df[col] > (df[col].mean() + 2.5*df[col].std()))].index)\n",
    "\n",
    "    # Standardisierung der Variablen in das Vielfache ihrer Standardabweichung \n",
    "    #df = pd.DataFrame(StandardScaler().fit_transform(df), index=df.index, columns=df.columns)\n",
    "\n",
    "    df.reset_index(drop=True).to_pickle(processed_dataset.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Bibliotheken des Kubeflow-Frameworks\n",
    "from kfp.dsl import Input, Output, Dataset, Model\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(\n",
    "    base_image='python:3.9.16',\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
    "def train(\n",
    "    processed_dataset: Input[Dataset],\n",
    "    trained_model: Output[Model]\n",
    "):\n",
    "    # Benötigte Bibliotheken innerhab der Komponente\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    # Ausgabe der vorherigen Komponente\n",
    "    df = pd.read_pickle(processed_dataset.path) \n",
    "\n",
    "    X = df[[\"wght\", \"year\"]] # unabhängige Variablen\n",
    "    y_true = df[\"mpg\"] # abhängige Variable\n",
    "    X_poly = PolynomialFeatures(degree=2).fit_transform(X)\n",
    "\n",
    "    poly = LinearRegression()\n",
    "    poly.fit(X_poly, y_true)\n",
    "\n",
    "    # Speichern für nachfolgende Komponenten\n",
    "    with open(trained_model.path,'wb') as f: \n",
    "        pickle.dump(poly,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import Input, Output, Dataset, Model, Artifact\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image='python:3.9.16',packages_to_install=[\"pandas\", \"scikit-learn\", \"matplotlib\"])\n",
    "def metrics(\n",
    "    trained_model: Input[Model],\n",
    "    processed_dataset: Input[Dataset],\n",
    "    metrics: Output[Artifact]\n",
    "):\n",
    "    from sklearn.metrics import PredictionErrorDisplay\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import pickle    \n",
    "    \n",
    "    with open(trained_model.path, 'rb') as f:\n",
    "        poly = pickle.load(f)\n",
    "\n",
    "    processed_dataset_df = pd.read_pickle(processed_dataset.path)\n",
    "    X = processed_dataset_df[[\"wght\", \"year\"]] # Ergebnis Feature Engineering: unabhängige Variablen\n",
    "    y_true = processed_dataset_df[\"mpg\"] # abhängige Variable \"Verbrauch\"  \n",
    "    X_poly = PolynomialFeatures(degree=2).fit_transform(X)\n",
    "\n",
    "    y_pred = poly.predict(X_poly)\n",
    "\n",
    "    # Ausgabe Bestimmtheitsmaß R^2\n",
    "    r_2 = poly.score(X_poly, y_true)\n",
    "\n",
    "    # Visueller Vergleich der wahren mit den vorhergesagten Werten \n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(6, 4))\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        subsample=100,\n",
    "        ax=axs[0],\n",
    "        random_state=0,\n",
    "    )\n",
    "    axs[0].yaxis.set_ticklabels([])\n",
    "    axs[0].xaxis.set_ticklabels([])\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"residual_vs_predicted\",\n",
    "        subsample=100,\n",
    "        ax=axs[1],\n",
    "        random_state=0,\n",
    "    )\n",
    "    axs[1].xaxis.set_ticklabels([])\n",
    "    axs[1].set_ylabel(\"Actual - Predicted \", labelpad=-3)\n",
    "    fig.suptitle(\"R^2 score: \" + str(r_2) , y=0.75)\n",
    "    plt.tight_layout(pad=4)\n",
    "    plt.savefig(metrics.path, format='png')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git + DVC push model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import Input, Output, Dataset, Model, Artifact\n",
    "from kfp import dsl, kubernetes\n",
    "\n",
    "@dsl.component(base_image='python:3.9.16',packages_to_install=[\"subprocess.run\",\"dvc[s3]\"])\n",
    "def git_dvc_push(\n",
    "    trigger: str,\n",
    "    timestamp: str,\n",
    "    trained_model: Model,\n",
    "    metrics: Artifact,\n",
    "    repo: Artifact\n",
    ") -> str:\n",
    "    import subprocess, os, shutil\n",
    "    import datetime\n",
    "\n",
    "    shutil.copy2(trained_model.path, os.path.join(repo.path, \"model.pkl\"))\n",
    "    shutil.copy2(metrics.path, os.path.join(repo.path, \"metrics.png\"))\n",
    "\n",
    "    subprocess.call([\"dvc\", \"add\", \"model.pkl\"], cwd=repo.path)\n",
    "    subprocess.call([\"dvc\", \"push\", \"model.pkl\"], cwd=repo.path)\n",
    "\n",
    "    subprocess.call([\"git\", \"config\", \"--global\", \"user.email\", '\"' + os.environ['GITHUB_USER_EMAIL'] + '\"'] , cwd=repo.path)\n",
    "    subprocess.call([\"git\", \"config\", \"--global\", \"user.name\",  '\"' + os.environ['GITHUB_USER_NAME'] + '\"'] , cwd=repo.path)\n",
    "   \n",
    "    subprocess.call([\"git\", \"add\", \"model.pkl.dvc\"] , cwd=repo.path)\n",
    "    subprocess.call([\"git\", \"add\", \"metrics.png\"] , cwd=repo.path)\n",
    "    subprocess.call([\"git\", \"commit\", \"-m\", \"[AUTO] Kubeflow Pipeline Execution triggered by [\" + trigger + \"] at [\" + timestamp + \"] \"] , cwd=repo.path)\n",
    "\n",
    "    timestamp_as_tag = datetime.datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S%z\").strftime('%Y%m%d%H%M%S')\n",
    "    subprocess.call([\"git\", \"tag\", \"-a\", timestamp_as_tag, \"-m\", trigger], cwd=repo.path)\n",
    "\n",
    "    gl_at = os.environ['GITLAB_ACCESS_TOKEN']\n",
    "    subprocess.call([\"git\", \"push\", \"https://<Git User>:\" + gl_at + \"@gitlab.com/<Git User>/ds-seminar-mlops-demo.git\", \"--follow-tags\", \"--force\"], cwd=repo.path) # TODO Change to create Tag\n",
    "    return \"True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.9.16',packages_to_install=[\"kubernetes\",\"kserve==0.11.1\"])\n",
    "def deploy(\n",
    "    # trained_model: Input[Model],\n",
    "):    \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    from kubernetes import client\n",
    "    from kubernetes.client.rest import ApiException \n",
    "    from kubernetes.client.models import V1EnvVar\n",
    "    from kubernetes.client import V1ResourceRequirements\n",
    "    from time import sleep\n",
    "\n",
    "    import shutil\n",
    "    import pathlib\n",
    "    import os\n",
    "    \n",
    "    namespace = \"kserve-deploy-test\" # utils.get_default_target_namespace()  \n",
    "    name='sklearn-mpg'\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    KServe = KServeClient()\n",
    "\n",
    "    try:\n",
    "        KServe.get(name, namespace=namespace, watch=True, timeout_seconds=10)\n",
    "        KServe.delete(name, namespace)\n",
    "    except RuntimeError:\n",
    "        pass # model does not exist yet\n",
    "    \n",
    "    found = False\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            KServe.get(name, namespace=namespace, timeout_seconds=20)\n",
    "            sleep(10)\n",
    "        except RuntimeError:\n",
    "            found = True\n",
    "            break # model does not exist yet ... \n",
    "        \n",
    "    if found == False:\n",
    "        raise Exception(\"Could not delete model\")\n",
    "\n",
    "    # TODO: remove??\n",
    "    aws_env_vars = [V1EnvVar(name=\"AWS_ACCESS_KEY_ID\", value=os.environ['AWS_ACCESS_KEY_ID']),\n",
    "                    V1EnvVar(name=\"AWS_SECRET_ACCESS_KEY\", value=os.environ['AWS_SECRET_ACCESS_KEY'])]\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                kind=constants.KSERVE_KIND,\n",
    "                                metadata=client.V1ObjectMeta(\n",
    "                                    name=name, \n",
    "                                    namespace=namespace, \n",
    "                                    annotations={\n",
    "                                        'sidecar.istio.io/inject':'false',\n",
    "                                        'serving.kserve.io/enable-prometheus-scraping': \"true\"\n",
    "                                        }),\n",
    "                                spec=V1beta1InferenceServiceSpec(\n",
    "                                predictor=V1beta1PredictorSpec(\n",
    "                                    service_account_name=\"sa-aws-kserve\",\n",
    "                                    sklearn=(\n",
    "                                        V1beta1SKLearnSpec(\n",
    "                                            storage_uri=\"s3://ds-seminar-model/model.pkl\",\n",
    "                                            resources=V1ResourceRequirements(\n",
    "                                                requests={'cpu': '200m', 'memory': '500Mi'},\n",
    "                                                limits={'cpu': '200m', 'memory': '500Mi'}\n",
    "                                                ),\n",
    "                                            env=aws_env_vars\n",
    "                                            )\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "    KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl, kubernetes, compiler\n",
    "from kfp.dsl import PipelineTask\n",
    "\n",
    "@dsl.pipeline\n",
    "def pipeline(trigger: str = \"\", timestamp: str = \"\"):\n",
    "    \n",
    "    git_dvc_clone_task = git_dvc_clone()\n",
    "    git_dvc_clone_task.set_caching_options(enable_caching=False)\n",
    "    kubernetes.use_secret_as_env(task=git_dvc_clone_task,secret_name=\"pipeline-secrets\",secret_key_to_env={\"AWS_ACCESS_KEY_ID\" : \"AWS_ACCESS_KEY_ID\" , \"AWS_SECRET_ACCESS_KEY\" : \"AWS_SECRET_ACCESS_KEY\", \"GITLAB_ACCESS_TOKEN\" : \"GITLAB_ACCESS_TOKEN\", \"GITHUB_USER_EMAIL\" : \"GITHUB_USER_EMAIL\", \"GITHUB_USER_NAME\" : \"GITHUB_USER_NAME\"})\n",
    "\n",
    "    data_preprocessing_task = data_preprocessing(dataset=git_dvc_clone_task.outputs['dataset'])\n",
    "\n",
    "    train_task = train(processed_dataset=data_preprocessing_task.outputs['processed_dataset'])\n",
    "\n",
    "    metrics_task = metrics(\n",
    "        trained_model=train_task.outputs['trained_model'],\n",
    "        processed_dataset=data_preprocessing_task.outputs['processed_dataset']\n",
    "    )\n",
    "\n",
    "    git_dvc_push_task = git_dvc_push(\n",
    "        trigger=trigger,\n",
    "        timestamp=timestamp,\n",
    "        repo=git_dvc_clone_task.outputs['repo'],\n",
    "        trained_model=train_task.outputs['trained_model'],\n",
    "        metrics=metrics_task.outputs['metrics']\n",
    "    )\n",
    "    kubernetes.use_secret_as_env(task=git_dvc_push_task,secret_name=\"pipeline-secrets\",secret_key_to_env={\"AWS_ACCESS_KEY_ID\" : \"AWS_ACCESS_KEY_ID\" , \"AWS_SECRET_ACCESS_KEY\" : \"AWS_SECRET_ACCESS_KEY\", \"GITLAB_ACCESS_TOKEN\" : \"GITLAB_ACCESS_TOKEN\", \"GITHUB_USER_EMAIL\" : \"GITHUB_USER_EMAIL\", \"GITHUB_USER_NAME\" : \"GITHUB_USER_NAME\"})\n",
    "    \n",
    "    with dsl.If(git_dvc_push_task.output == \"True\"):    \n",
    "        deployment_task = deploy()\n",
    "        deployment_task.set_caching_options(enable_caching=False)\n",
    "        kubernetes.use_secret_as_env(task=deployment_task,secret_name=\"pipeline-secrets\",secret_key_to_env={\"AWS_ACCESS_KEY_ID\" : \"AWS_ACCESS_KEY_ID\" , \"AWS_SECRET_ACCESS_KEY\" : \"AWS_SECRET_ACCESS_KEY\", \"GITLAB_ACCESS_TOKEN\" : \"GITLAB_ACCESS_TOKEN\", \"GITHUB_USER_EMAIL\" : \"GITHUB_USER_EMAIL\", \"GITHUB_USER_NAME\" : \"GITHUB_USER_NAME\"})\n",
    "\n",
    "compiler.Compiler().compile(pipeline, package_path='pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
